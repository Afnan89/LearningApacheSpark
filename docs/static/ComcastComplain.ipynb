{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connecting to spark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "## set up spark context\n",
    "conf = SparkConf().setAppName(\"myApp\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# create sparksession object\n",
    "from pyspark.sql import SparkSession\n",
    "sparksession = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import preproc as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())\n",
    "string_to_float_udf = udf(pp.string_to_float, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sparksession.read.csv(\"data/comcastcomplaints/comcast2000.csv\",\n",
    "                           header=True,\n",
    "                          inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+\n",
      "|              author|    posted_on|rating|                text|\n",
      "+--------------------+-------------+------+--------------------+\n",
      "|Alantae of Cheste...|Nov. 22, 2016|     1|I used to love Co...|\n",
      "|Vera of Philadelp...|Nov. 19, 2016|     1|I'm so over Comca...|\n",
      "|Sarah of Rancho C...|Nov. 17, 2016|     1|If I could give t...|\n",
      "|Dennis of Manches...|Nov. 16, 2016|     1|I've had the wors...|\n",
      "|Ryan of Bellevue, WA|Nov. 14, 2016|     1|Check your contra...|\n",
      "+--------------------+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "#lang_df = df.withColumn(\"lang\", check_lang_udf(df['text']))\n",
    "#lang_df.show(5)\n",
    "#en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")\n",
    "\n",
    "en_df = df.filter(df.text != '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "|              author|    posted_on|rating|                text|           stop_text|\n",
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "|Alantae of Cheste...|Nov. 22, 2016|     1|I used to love Co...|I used love Comca...|\n",
      "|Vera of Philadelp...|Nov. 19, 2016|     1|I'm so over Comca...|I'm Comcast! The ...|\n",
      "|Sarah of Rancho C...|Nov. 17, 2016|     1|If I could give t...|If I could give n...|\n",
      "|Dennis of Manches...|Nov. 16, 2016|     1|I've had the wors...|I've worst experi...|\n",
      "|Ryan of Bellevue, WA|Nov. 14, 2016|     1|Check your contra...|Check contract si...|\n",
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df = en_df.withColumn(\"stop_text\", remove_stops_udf(\"text\"))\n",
    "rm_stops_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "|              author|    posted_on|rating|                text|           stop_text|\n",
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "|Alantae of Cheste...|Nov. 22, 2016|     1|I used to love Co...|I used love Comca...|\n",
      "|Vera of Philadelp...|Nov. 19, 2016|     1|I'm so over Comca...|I'm Comcast! The ...|\n",
      "|Sarah of Rancho C...|Nov. 17, 2016|     1|If I could give t...|If I could give n...|\n",
      "|Dennis of Manches...|Nov. 16, 2016|     1|I've had the wors...|I've worst experi...|\n",
      "|Ryan of Bellevue, WA|Nov. 14, 2016|     1|Check your contra...|Check contract si...|\n",
      "+--------------------+-------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove stop words to reduce dimensionality\n",
    "\n",
    "rm_stops_df = en_df.withColumn(\"stop_text\", remove_stops_udf(\"text\"))\n",
    "rm_stops_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+\n",
      "|              author|    posted_on|rating|                text|           stop_text|           feat_text|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+\n",
      "|Alantae of Cheste...|Nov. 22, 2016|     1|I used to love Co...|I used love Comca...|  used love comca...|\n",
      "|Vera of Philadelp...|Nov. 19, 2016|     1|I'm so over Comca...|I'm Comcast! The ...|   comcast the wo...|\n",
      "|Sarah of Rancho C...|Nov. 17, 2016|     1|If I could give t...|If I could give n...|   could give neg...|\n",
      "|Dennis of Manches...|Nov. 16, 2016|     1|I've had the wors...|I've worst experi...|   worst experien...|\n",
      "|Ryan of Bellevue, WA|Nov. 14, 2016|     1|Check your contra...|Check contract si...|check contract si...|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"feat_text\", remove_features_udf(rm_stops_df[\"stop_text\"]))\n",
    "rm_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         tagged_text|\n",
      "+--------------------+\n",
      "| used love comcas...|\n",
      "| comcast worst in...|\n",
      "| give negative st...|\n",
      "| worst experience...|\n",
      "| check contract s...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df[\"feat_text\"]))\n",
    "tagged_df.select(['tagged_text']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           lemm_text|\n",
      "+--------------------+\n",
      "|use love comcast ...|\n",
      "|comcast worst int...|\n",
      "|give negative sta...|\n",
      "|worst experience ...|\n",
      "|check contract si...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))\n",
    "lemm_df.select(['lemm_text']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|              author|    posted_on|rating|                text|           stop_text|           feat_text|         tagged_text|           lemm_text|is_blank|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|Alantae of Cheste...|Nov. 22, 2016|     1|I used to love Co...|I used love Comca...|  used love comca...| used love comcas...|use love comcast ...|   False|\n",
      "|Vera of Philadelp...|Nov. 19, 2016|     1|I'm so over Comca...|I'm Comcast! The ...|   comcast the wo...| comcast worst in...|comcast worst int...|   False|\n",
      "|Sarah of Rancho C...|Nov. 17, 2016|     1|If I could give t...|If I could give n...|   could give neg...| give negative st...|give negative sta...|   False|\n",
      "|Dennis of Manches...|Nov. 16, 2016|     1|I've had the wors...|I've worst experi...|   worst experien...| worst experience...|worst experience ...|   False|\n",
      "|Ryan of Bellevue, WA|Nov. 14, 2016|     1|Check your contra...|Check contract si...|check contract si...| check contract s...|check contract si...|   False|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========remove all rows containing only blank spaces============\n",
    "check_blanks_df_tmp = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "check_blanks_df_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_blanks_df = check_blanks_df_tmp\n",
    "\n",
    "# rename columns df.withColumnRenamed(\"colName\", \"newColName\")\n",
    "rename_df= no_blanks_df.withColumn(\"lemm_text\", no_blanks_df[\"text\"]).\\\n",
    "             withColumn(\"label\", string_to_float_udf(\"rating\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|              author|    posted_on|rating|                text|           stop_text|           feat_text|         tagged_text|           lemm_text|is_blank|label|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|Amie of Richmond, VA|        38111|     0|Comcast has been ...|Comcast constant ...|comcast constant ...| comcast constant...|Comcast has been ...|   False|  0.0|\n",
      "|Ana-Maria of Cora...|Dec. 11, 2015|     1|\"I pay $140 month...|\"I pay $140 month...|  pay monthly int...| pay monthly inte...|\"I pay $140 month...|   False|  1.0|\n",
      "|Charles of Mt Ple...|Oct. 26, 2015|     1|\"In July of 2015 ...|\"In July 2015 I m...|  july  moved bel...| moved bellevue p...|\"In July of 2015 ...|   False|  1.0|\n",
      "|Cynthia of Shreve...|Aug. 17, 2011|     1|Since April 28th ...|Since April 28th ...|since april   not...| april nothing pr...|Since April 28th ...|   False|  1.0|\n",
      "|Diane of Loganvil...| Oct. 7, 2008|     0|For 6 years, I've...|For 6 years, I've...|for years   worke...| years worked hom...|For 6 years, I've...|   False|  0.0|\n",
      "+--------------------+-------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dedupe important since alot of the tweets only differed by url's and RT mentions\n",
    "dedup_df = rename_df.dropDuplicates(['author', 'text'])\n",
    "\n",
    "dedup_df.show(5)\n",
    "type(dedup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|              author|label|                text|\n",
      "+--------------------+-----+--------------------+\n",
      "|Stephen of Miami, FL|  1.0|I am writing on b...|\n",
      "|Ana-Maria of Cora...|  1.0|\"I pay $140 month...|\n",
      "|Charles of Mt Ple...|  1.0|\"In July of 2015 ...|\n",
      "|Jason of Rancoh M...|  1.0|I attempted to in...|\n",
      "|Federico of Chica...|  1.0|Comcast charged m...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only the columns we care about\n",
    "data_set = dedup_df.select([\"author\",\"label\",\"text\"])\n",
    "\n",
    "data_set.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split training & validation sets with 60% to training and use a seed value of 1987\n",
    "(training_df,test_df) = data_set.randomSplit([0.6, 0.4])\n",
    "training_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nb = NaiveBayes()\n",
    "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, rf])\n",
    "#rf = RandomForestClassifier(numTrees=100,maxDepth=20, seed=42)\n",
    "rf = RandomForestClassifier()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.maxDepth,[4,8,10]).\\\n",
    "                    addGrid(rf.impurity, ['entropy','gini']).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "                    \n",
    "\n",
    "#training_df.show(5)  \n",
    "cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_path = '/Users/wenqiangfeng/Dropbox/Spark/Code/model'\n",
    "modelPath = temp_path + \"/Comcast_model\"\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel = bestModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6688963210702342"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(cvModel.transform(training_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876447534728403"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(cvModel.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
