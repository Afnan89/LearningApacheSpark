{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark for RandomForest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up spark context and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark RandomForest Regression example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').load(\"../data/Advertising.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Convert to float format\n",
    "# def string_to_float(x):\n",
    "#     return float(x)\n",
    "\n",
    "\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType, DoubleType\n",
    "# string_to_float_udf = udf(string_to_float, DoubleType())\n",
    "\n",
    "# df = df.withColumn(\"horsepower\", string_to_float_udf(\"horsepower\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the data to dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the dataset and the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the data to dense vector\n",
    "#def transData(row):\n",
    "#    return Row(label=row[\"Sales\"],\n",
    "#               features=Vectors.dense([row[\"TV\"],\n",
    "#                                       row[\"Radio\"],\n",
    "#                                       row[\"Newspaper\"]]))\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform the dataset to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transformed = df.rdd.map(transData).toDF() \n",
    "transformed= transData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deal With Categorical Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = featureIndexer.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the data to training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----------------+\n",
      "|        features|label| indexedFeatures|\n",
      "+----------------+-----+----------------+\n",
      "|  [0.7,39.6,8.7]|  1.6|  [0.7,39.6,8.7]|\n",
      "|   [8.6,2.1,1.0]|  4.8|   [8.6,2.1,1.0]|\n",
      "| [8.7,48.9,75.0]|  7.2| [8.7,48.9,75.0]|\n",
      "|[11.7,36.9,45.2]|  7.3|[11.7,36.9,45.2]|\n",
      "|[13.2,15.9,49.6]|  5.6|[13.2,15.9,49.6]|\n",
      "+----------------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+---------------+\n",
      "|       features|label|indexedFeatures|\n",
      "+---------------+-----+---------------+\n",
      "| [4.1,11.6,5.7]|  3.2| [4.1,11.6,5.7]|\n",
      "| [5.4,29.9,9.4]|  5.3| [5.4,29.9,9.4]|\n",
      "|[7.3,28.1,41.4]|  5.5|[7.3,28.1,41.4]|\n",
      "|[7.8,38.9,50.6]|  6.6|[7.8,38.9,50.6]|\n",
      "| [8.4,27.2,2.1]|  5.7| [8.4,27.2,2.1]|\n",
      "+---------------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fit RandomForest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "rf = RandomForestRegressor() #numTrees=2, maxDepth=2, seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Pipeline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Summary of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def modelsummary(model):\n",
    "#     import numpy as np\n",
    "#     print (\"Note: the last rows are the information for Intercept\")\n",
    "#     print (\"##\",\"-------------------------------------------------\")\n",
    "#     print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "#     coef = np.append(list(model.coefficients),model.intercept)\n",
    "#     Summary=model.summary\n",
    "    \n",
    "#     for i in range(len(Summary.pValues)):\n",
    "#         print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "#         '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "#         '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "#         '{:10.6f}'.format(Summary.pValues[i]))\n",
    "        \n",
    "#     print (\"##\",'---')\n",
    "#     print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "#            % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "#            % Summary.rootMeanSquaredError )\n",
    "#     print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", \\\n",
    "#             Total iterations: %i\"% Summary.totalIterations)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------------------+\n",
      "|       features|label|        prediction|\n",
      "+---------------+-----+------------------+\n",
      "| [4.1,11.6,5.7]|  3.2| 8.155439814814816|\n",
      "| [5.4,29.9,9.4]|  5.3|10.412769901394899|\n",
      "|[7.3,28.1,41.4]|  5.5| 12.13735648148148|\n",
      "|[7.8,38.9,50.6]|  6.6|11.321796703296704|\n",
      "| [8.4,27.2,2.1]|  5.7|12.071421957671957|\n",
      "+---------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.35912\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.831\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics \n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {:4.3f}'.format(r2_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 0.4994, 1: 0.3196, 2: 0.181})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressionModel (uid=dtr_c75f1c75442c) of depth 5 with 43 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_70fc2d441581) of depth 5 with 45 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_bc8464f545a7) of depth 5 with 31 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_a8a7e5367154) of depth 5 with 59 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_3ea01314fcbc) of depth 5 with 47 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_be9a04ac22a6) of depth 5 with 45 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_38610d47328a) of depth 5 with 51 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_bf14aea0ad3b) of depth 5 with 49 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_cde24ebd6bb6) of depth 5 with 39 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_a1fc9bd4fbeb) of depth 5 with 57 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_37798d6db1ba) of depth 5 with 41 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_c078b73ada63) of depth 5 with 41 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_fd00e3a070ad) of depth 5 with 55 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_9d01d5fb8604) of depth 5 with 45 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_8bd8bdddf642) of depth 5 with 41 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_e53b7bae30f8) of depth 5 with 49 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_808a869db21c) of depth 5 with 47 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_64d0916bceb0) of depth 5 with 33 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_0891055fff94) of depth 5 with 55 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_19c8bbad26c2) of depth 5 with 51 nodes]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
