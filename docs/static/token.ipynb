{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark NLP demo\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter = spark.createDataFrame([\n",
    "                                ('George is a spark expert', 'George', 1.0),\n",
    "                                ('Jack is learning spark', 'Jack', 0.0)],\n",
    "                                ['text', 'id', 'label']\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-----+\n",
      "|text                    |id    |label|\n",
      "+------------------------+------+-----+\n",
      "|George is a spark expert|George|1.0  |\n",
      "|Jack is learning spark  |Jack  |0.0  |\n",
      "+------------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-----+------------------------------+\n",
      "|text                    |id    |label|tokens                        |\n",
      "+------------------------+------+-----+------------------------------+\n",
      "|George is a spark expert|George|1.0  |[george, is, a, spark, expert]|\n",
      "|Jack is learning spark  |Jack  |0.0  |[jack, is, learning, spark]   |\n",
      "+------------------------+------+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_mod = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "twitter_tokens = tokenizer_mod.transform(twitter)\n",
    "twitter_tokens.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF_mod = HashingTF(numFeatures=pow(2,4), inputCol='tokens', \\\n",
    "                          outputCol='Features(vocab_size,[index],[tf]')\n",
    "hashingTF_twitter = hashingTF_mod.transform(twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-----+------------------------------+--------------------------------+\n",
      "|text                    |id    |label|tokens                        |Features(vocab_size,[index],[tf]|\n",
      "+------------------------+------+-----+------------------------------+--------------------------------+\n",
      "|George is a spark expert|George|1.0  |[george, is, a, spark, expert]|(16,[1,2,3,9],[2.0,1.0,1.0,1.0])|\n",
      "|Jack is learning spark  |Jack  |0.0  |[jack, is, learning, spark]   |(16,[0,1,3],[1.0,2.0,1.0])      |\n",
      "+------------------------+------+-----+------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF_twitter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can not get the vocabulary. Since hashing is non-injective there is no inverse function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(vocabSize=pow(2,4), inputCol='tokens', outputCol='features')\n",
    "countVectorizer_mod = count_vectorizer.fit(twitter_tokens)\n",
    "countVectorizer_twitter = countVectorizer_mod.transform(twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-----+------------------------------+-------------------------------------+\n",
      "|text                    |id    |label|tokens                        |features                             |\n",
      "+------------------------+------+-----+------------------------------+-------------------------------------+\n",
      "|George is a spark expert|George|1.0  |[george, is, a, spark, expert]|(7,[0,1,3,4,5],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|Jack is learning spark  |Jack  |0.0  |[jack, is, learning, spark]   |(7,[0,1,2,6],[1.0,1.0,1.0,1.0])      |\n",
      "+------------------------+------+-----+------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countVectorizer_twitter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'spark', 'learning', 'a', 'expert', 'george', 'jack']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVectorizer_mod.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another good demo from Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of hashingTF function\n",
      "+------------------------------------------+---------------------------------------------------------+\n",
      "|words                                     |Features(vocab_size,[index],[tf])                        |\n",
      "+------------------------------------------+---------------------------------------------------------+\n",
      "|[hi, i, heard, about, spark]              |(100,[5,29,57,60,77],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "|[i, wish, java, could, use, case, classes]|(100,[9,13,29,42,67,89,95],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[logistic, regression, models, are, neat] |(100,[4,38,86,93,95],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "+------------------------------------------+---------------------------------------------------------+\n",
      "\n",
      "Out of CountVectorizer function\n",
      "+------------------------------------------+-----------------------------------------------------+\n",
      "|words                                     |Features(vocab_size,[index],[tf])                    |\n",
      "+------------------------------------------+-----------------------------------------------------+\n",
      "|[hi, i, heard, about, spark]              |(16,[0,1,6,7,8],[1.0,1.0,1.0,1.0,1.0])               |\n",
      "|[i, wish, java, could, use, case, classes]|(16,[0,2,4,9,11,12,15],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[logistic, regression, models, are, neat] |(16,[3,5,10,13,14],[1.0,1.0,1.0,1.0,1.0])            |\n",
      "+------------------------------------------+-----------------------------------------------------+\n",
      "\n",
      "Vocabulary from CountVectorizerModel is \n",
      "['i', 'heard', 'classes', 'neat', 'java', 'models', 'spark', 'hi', 'about', 'could', 'regression', 'wish', 'use', 'are', 'logistic', 'case']\n"
     ]
    }
   ],
   "source": [
    "## http://stackoverflow.com/questions/35205865/what-is-the-difference-between-hashingtf-and-countvectorizer-in-spark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")],\n",
    " [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"Features\", numFeatures=100)\n",
    "hashingTF_model = hashingTF.transform(wordsData)\n",
    "print(\"Out of hashingTF function\")\n",
    "hashingTF_model.select('words',hashingTF_model.Features.alias('Features(vocab_size,[index],[tf])')).show(truncate=False)\n",
    "\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"Features\", vocabSize=20)\n",
    "\n",
    "cv_model = cv.fit(wordsData)\n",
    "\n",
    "cv_result = cv_model.transform(wordsData)\n",
    "print(\"Out of CountVectorizer function\")\n",
    "cv_result.select('words',cv_result.Features.alias('Features(vocab_size,[index],[tf])')).show(truncate=False)\n",
    "print(\"Vocabulary from CountVectorizerModel is \\n\" + str(cv_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
