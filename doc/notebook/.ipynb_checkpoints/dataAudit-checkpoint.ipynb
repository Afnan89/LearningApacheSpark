{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python data audit example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|  y|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular|      79|       1|   -1|       0| no|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular|     220|       1|  339|       4| no|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular|     185|       1|  330|       1| no|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|     199|       4|   -1|       0| no|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|     226|       1|   -1|       0| no|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(\"../data/bank.csv\",header=True);\n",
    "df.drop('day','month','poutcome').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [f.dataType for f in df.schema.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'int'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('balance', 'int'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'string'),\n",
       " ('duration', 'int'),\n",
       " ('campaign', 'int'),\n",
       " ('pdays', 'int'),\n",
       " ('previous', 'int'),\n",
       " ('poutcome', 'string'),\n",
       " ('y', 'string')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = spark.createDataFrame(df.dtypes).toDF('Names','Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|    Names| Types|\n",
      "+---------+------+\n",
      "|      age|   int|\n",
      "|      job|string|\n",
      "|  marital|string|\n",
      "|education|string|\n",
      "|  default|string|\n",
      "|  balance|   int|\n",
      "|  housing|string|\n",
      "|     loan|string|\n",
      "|  contact|string|\n",
      "|      day|   int|\n",
      "|    month|string|\n",
      "| duration|   int|\n",
      "| campaign|   int|\n",
      "|    pdays|   int|\n",
      "| previous|   int|\n",
      "| poutcome|string|\n",
      "|        y|string|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|    job|\n",
      "+-------+-------+\n",
      "|  count|   4521|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min| admin.|\n",
      "|    max|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# get variable name and types\n",
    "out = spark.createDataFrame(df.dtypes).toDF('Names','Types')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"mean\", LongType(), True), \n",
    "    StructField(\"min\", LongType(), False), \n",
    "    StructField(\"max\", LongType(), False)])\n",
    "\n",
    "schema_freq = StructType([\n",
    "    StructField(\"freqItems\", LongType(), True)])\n",
    "\n",
    "df_stats = spark.createDataFrame([],schema)\n",
    "df_freq = spark.createDataFrame([],schema_freq)\n",
    "\n",
    "\n",
    "for i in df.columns:\n",
    "\n",
    "    des_d = df.select([mean(i).alias('mean'), min(i).alias('min'), max(i).alias('max')])\n",
    "    #des_d.printSchema()\n",
    "    freq = df.stat.freqItems([i], 0.3)\n",
    "    freq = freq.withColumn('freqItems',col(i+'_freqItems')).select('freqItems')\n",
    "    #freq.show(1,False)\n",
    "    #freq.printSchema()\n",
    "    \n",
    "    df_stats = df_stats.union(des_d)\n",
    "    #df_freq = df_freq.union(freq)\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "def add_columns(self, list_of_tuples):\n",
    "    \n",
    "    for col in list_of_tuples:\n",
    "\n",
    "        prev_col = col[0]\n",
    "\n",
    "        if isinstance(prev_col, list):\n",
    "            cols = [self[j] if isinstance(j, str) else j for j in prev_col]\n",
    "\n",
    "            for new_col, func in zip(col[1], col[2]):\n",
    "                self = self.withColumn(new_col, func(*cols))\n",
    "\n",
    "        else:\n",
    "\n",
    "            for new_col, func in zip(col[1], col[2]):\n",
    "                col = self[prev_col] if isinstance(prev_col, str) else prev_col\n",
    "                self = self.withColumn(new_col, func(col))\n",
    "                \n",
    "    return self    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------+\n",
      "|              mean|     min|    max|\n",
      "+------------------+--------+-------+\n",
      "| 41.17009511170095|      19|     87|\n",
      "|              null|  admin.|unknown|\n",
      "|              null|divorced| single|\n",
      "|              null| primary|unknown|\n",
      "|              null|      no|    yes|\n",
      "|1422.6578190665782|   -3313|  71188|\n",
      "|              null|      no|    yes|\n",
      "|              null|      no|    yes|\n",
      "|              null|cellular|unknown|\n",
      "|15.915284229152842|       1|     31|\n",
      "|              null|     apr|    sep|\n",
      "|263.96129174961294|       4|   3025|\n",
      "| 2.793629727936297|       1|     50|\n",
      "|39.766644547666445|      -1|    871|\n",
      "|0.5425790754257908|       0|     25|\n",
      "|              null| failure|unknown|\n",
      "|              null|      no|    yes|\n",
      "+------------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+\n",
      "|age_default| no|yes|\n",
      "+-----------+---+---+\n",
      "|         69|  6|  0|\n",
      "|         56| 72|  2|\n",
      "|         42|138|  3|\n",
      "|         24| 23|  1|\n",
      "|         37|158|  3|\n",
      "|         25| 43|  1|\n",
      "|         52| 86|  0|\n",
      "|         20|  3|  0|\n",
      "|         46|119|  0|\n",
      "|         57| 87|  4|\n",
      "|         78|  3|  0|\n",
      "|         29| 97|  0|\n",
      "|         84|  1|  0|\n",
      "|         61| 16|  0|\n",
      "|         74|  3|  0|\n",
      "|         60| 47|  0|\n",
      "|         28|102|  1|\n",
      "|         38|158|  1|\n",
      "|         70|  7|  0|\n",
      "|         21|  7|  0|\n",
      "+-----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age','default').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "|  1|  2|  3|\n",
      "| 11| 22|  3|\n",
      "|  1|  2|  3|\n",
      "| 13| 26|  1|\n",
      "|  1|  2|  3|\n",
      "| 15| 30|  3|\n",
      "|  1|  2|  3|\n",
      "| 17| 34|  1|\n",
      "|  1|  2|  3|\n",
      "| 19| 38|  3|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|a_freqItems|b_freqItems|c_freqItems|\n",
      "+-----------+-----------+-----------+\n",
      "|    [11, 1]|    [2, 22]|     [1, 3]|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given the above DataFrame, the following code finds the\n",
    "# frequent items that show up 40% of the time for each column:\n",
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.5)\n",
    "freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  b|count|\n",
      "+---+-----+\n",
      "|  2|   51|\n",
      "|138|    1|\n",
      "| 26|    1|\n",
      "|126|    1|\n",
      "|130|    1|\n",
      "| 54|    1|\n",
      "| 22|    1|\n",
      "|198|    1|\n",
      "| 50|    1|\n",
      "| 94|    1|\n",
      "| 98|    1|\n",
      "| 58|    1|\n",
      "|  6|    1|\n",
      "|110|    1|\n",
      "|190|    1|\n",
      "|158|    1|\n",
      "|178|    1|\n",
      "|150|    1|\n",
      "|146|    1|\n",
      "|170|    1|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('b').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|myCol|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "+-----+\n",
      "\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "| 20|\n",
      "+---+\n",
      "\n",
      "+-----+\n",
      "|myCol|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|   20|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstDF = spark.range(3).toDF(\"myCol\")\n",
    "firstDF.show()\n",
    "newRow = spark.createDataFrame([[20]])\n",
    "newRow.show()\n",
    "appended = firstDF.union(newRow)\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
