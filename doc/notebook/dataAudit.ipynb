{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python data audit example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|  y|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular|      79|       1|   -1|       0| no|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular|     220|       1|  339|       4| no|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular|     185|       1|  330|       1| no|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|     199|       4|   -1|       0| no|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|     226|       1|   -1|       0| no|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(\"../data/bank.csv\",header=True);\n",
    "df.drop('day','month','poutcome').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-654dd2f9cb3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dt216661/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 973\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [f.dataType for f in df.schema.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'int'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('balance', 'int'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'string'),\n",
       " ('duration', 'int'),\n",
       " ('campaign', 'int'),\n",
       " ('pdays', 'int'),\n",
       " ('previous', 'int'),\n",
       " ('poutcome', 'string'),\n",
       " ('y', 'string')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = spark.createDataFrame(df.dtypes).toDF('Names','Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|    Names| Types|\n",
      "+---------+------+\n",
      "|      age|   int|\n",
      "|      job|string|\n",
      "|  marital|string|\n",
      "|education|string|\n",
      "|  default|string|\n",
      "|  balance|   int|\n",
      "|  housing|string|\n",
      "|     loan|string|\n",
      "|  contact|string|\n",
      "|      day|   int|\n",
      "|    month|string|\n",
      "| duration|   int|\n",
      "| campaign|   int|\n",
      "|    pdays|   int|\n",
      "| previous|   int|\n",
      "| poutcome|string|\n",
      "|        y|string|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|    job|\n",
      "+-------+-------+\n",
      "|  count|   4521|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min| admin.|\n",
      "|    max|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33.0, 39.0, 49.0]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quantiles = df.stat.approxQuantile('age',(0.25,0.5,0.75),0.0)\n",
    "Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[57]     |\n",
      "+---------+\n",
      "\n",
      "+---------------------------------------+\n",
      "|freqItems                              |\n",
      "+---------------------------------------+\n",
      "|[management, entrepreneur, blue-collar]|\n",
      "+---------------------------------------+\n",
      "\n",
      "+---------------------------+\n",
      "|freqItems                  |\n",
      "+---------------------------+\n",
      "|[married, divorced, single]|\n",
      "+---------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|freqItems                     |\n",
      "+------------------------------+\n",
      "|[tertiary, secondary, primary]|\n",
      "+------------------------------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[no, yes]|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[1136]   |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[no, yes]|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[no, yes]|\n",
      "+---------+\n",
      "\n",
      "+------------------------------+\n",
      "|freqItems                     |\n",
      "+------------------------------+\n",
      "|[cellular, telephone, unknown]|\n",
      "+------------------------------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[3]      |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[may]    |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[345]    |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[2, 4, 1]|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[-1]     |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[1, 7, 0]|\n",
      "+---------+\n",
      "\n",
      "+-------------------------+\n",
      "|freqItems                |\n",
      "+-------------------------+\n",
      "|[failure, unknown, other]|\n",
      "+-------------------------+\n",
      "\n",
      "+---------+\n",
      "|freqItems|\n",
      "+---------+\n",
      "|[no, yes]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# get variable name and types\n",
    "out = spark.createDataFrame(df.dtypes).toDF('Names','Types')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"mean\", LongType(), True), \n",
    "    StructField(\"min\", LongType(), False), \n",
    "    StructField(\"max\", LongType(), False)])\n",
    "\n",
    "schema_freq = StructType([\n",
    "    StructField(\"freqItems\", LongType(), True)])\n",
    "\n",
    "df_stats = spark.createDataFrame([],schema)\n",
    "df_freq = spark.createDataFrame([],schema_freq)\n",
    "\n",
    "\n",
    "for i in df.columns:\n",
    "\n",
    "    des_d = df.select([mean(i).alias('mean'), min(i).alias('min'), max(i).alias('max')])\n",
    "    #des_d.printSchema()\n",
    "    freq = df.stat.freqItems([i], 0.3)\n",
    "#     Quantiles = df.stat.approxQuantile(i,(0.25,0.5,0.75),0.0)\n",
    "#     print(Quantiles)\n",
    "    freq = freq.withColumn('freqItems',col(i+'_freqItems')).select('freqItems')\n",
    "    freq.show(1,False)\n",
    "    #freq.printSchema()\n",
    "    \n",
    "    df_stats = df_stats.union(des_d)\n",
    "    #df_freq = df_freq.union(freq)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------+\n",
      "|              mean|     min|    max|\n",
      "+------------------+--------+-------+\n",
      "| 41.17009511170095|      19|     87|\n",
      "|              null|  admin.|unknown|\n",
      "|              null|divorced| single|\n",
      "|              null| primary|unknown|\n",
      "|              null|      no|    yes|\n",
      "|1422.6578190665782|   -3313|  71188|\n",
      "|              null|      no|    yes|\n",
      "|              null|      no|    yes|\n",
      "|              null|cellular|unknown|\n",
      "|15.915284229152842|       1|     31|\n",
      "|              null|     apr|    sep|\n",
      "|263.96129174961294|       4|   3025|\n",
      "| 2.793629727936297|       1|     50|\n",
      "|39.766644547666445|      -1|    871|\n",
      "|0.5425790754257908|       0|     25|\n",
      "|              null| failure|unknown|\n",
      "|              null|      no|    yes|\n",
      "+------------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def unequal_union_sdf(sdf1, sdf2):\n",
    "    s_df1_schema = set((x.name, x.dataType) for x in sdf1.schema)\n",
    "    s_df2_schema = set((x.name, x.dataType) for x in sdf2.schema)\n",
    "\n",
    "    for i,j in s_df2_schema.difference(s_df1_schema):\n",
    "        sdf1 = sdf1.withColumn(i,F.lit(None).cast(j))\n",
    "\n",
    "    for i,j in s_df1_schema.difference(s_df2_schema):\n",
    "        sdf2 = sdf2.withColumn(i,F.lit(None).cast(j))\n",
    "\n",
    "    common_schema_colnames = sdf1.columns\n",
    "    sdk = \\\n",
    "        sdf1.select(common_schema_colnames).union(sdf2.select(common_schema_colnames))\n",
    "    return sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf_concat = unequal_union_sdf(out,df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------------+-------+--------+\n",
      "|    Names| Types|             mean|    max|     min|\n",
      "+---------+------+-----------------+-------+--------+\n",
      "|      age|   int|             null|   null|    null|\n",
      "|      job|string|             null|   null|    null|\n",
      "|  marital|string|             null|   null|    null|\n",
      "|education|string|             null|   null|    null|\n",
      "|  default|string|             null|   null|    null|\n",
      "|  balance|   int|             null|   null|    null|\n",
      "|  housing|string|             null|   null|    null|\n",
      "|     loan|string|             null|   null|    null|\n",
      "|  contact|string|             null|   null|    null|\n",
      "|      day|   int|             null|   null|    null|\n",
      "|    month|string|             null|   null|    null|\n",
      "| duration|   int|             null|   null|    null|\n",
      "| campaign|   int|             null|   null|    null|\n",
      "|    pdays|   int|             null|   null|    null|\n",
      "| previous|   int|             null|   null|    null|\n",
      "| poutcome|string|             null|   null|    null|\n",
      "|        y|string|             null|   null|    null|\n",
      "|     null|  null|41.17009511170095|     87|      19|\n",
      "|     null|  null|             null|unknown|  admin.|\n",
      "|     null|  null|             null| single|divorced|\n",
      "+---------+------+-----------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+\n",
      "|age_default| no|yes|\n",
      "+-----------+---+---+\n",
      "|         69|  6|  0|\n",
      "|         56| 72|  2|\n",
      "|         42|138|  3|\n",
      "|         24| 23|  1|\n",
      "|         37|158|  3|\n",
      "|         25| 43|  1|\n",
      "|         52| 86|  0|\n",
      "|         20|  3|  0|\n",
      "|         46|119|  0|\n",
      "|         57| 87|  4|\n",
      "|         78|  3|  0|\n",
      "|         29| 97|  0|\n",
      "|         84|  1|  0|\n",
      "|         61| 16|  0|\n",
      "|         74|  3|  0|\n",
      "|         60| 47|  0|\n",
      "|         28|102|  1|\n",
      "|         38|158|  1|\n",
      "|         70|  7|  0|\n",
      "|         21|  7|  0|\n",
      "+-----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age','default').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "|  1|  2|  3|\n",
      "| 11| 22|  3|\n",
      "|  1|  2|  3|\n",
      "| 13| 26|  1|\n",
      "|  1|  2|  3|\n",
      "| 15| 30|  3|\n",
      "|  1|  2|  3|\n",
      "| 17| 34|  1|\n",
      "|  1|  2|  3|\n",
      "| 19| 38|  3|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n",
      "|        a_freqItems|         b_freqItems|c_freqItems|\n",
      "+-------------------+--------------------+-----------+\n",
      "|[23, 59, 47, 71, 1]|[2, 142, 94, 46, ...|     [1, 3]|\n",
      "+-------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given the above DataFrame, the following code finds the\n",
    "# frequent items that show up 40% of the time for each column:\n",
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.2)\n",
    "freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  1|   51|\n",
      "|  5|    1|\n",
      "| 29|    1|\n",
      "| 19|    1|\n",
      "| 57|    1|\n",
      "| 43|    1|\n",
      "| 31|    1|\n",
      "|  7|    1|\n",
      "| 77|    1|\n",
      "| 25|    1|\n",
      "| 39|    1|\n",
      "| 95|    1|\n",
      "| 71|    1|\n",
      "|  9|    1|\n",
      "| 27|    1|\n",
      "| 63|    1|\n",
      "| 51|    1|\n",
      "| 17|    1|\n",
      "| 79|    1|\n",
      "| 41|    1|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('a').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|myCol|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "+-----+\n",
      "\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "| 20|\n",
      "+---+\n",
      "\n",
      "+-----+\n",
      "|myCol|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|   20|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstDF = spark.range(3).toDF(\"myCol\")\n",
    "firstDF.show()\n",
    "newRow = spark.createDataFrame([[20]])\n",
    "newRow.show()\n",
    "appended = firstDF.union(newRow)\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "import time\n",
    "\n",
    "def quantile(rdd, p, sample=None, seed=None):\n",
    "    \"\"\"Compute a quantile of order p ∈ [0, 1]\n",
    "    :rdd a numeric rdd\n",
    "    :p quantile(between 0 and 1)\n",
    "    :sample fraction of and rdd to use. If not provided we use a whole dataset\n",
    "    :seed random number generator seed to be used with sample\n",
    "    \"\"\"\n",
    "    assert 0 <= p <= 1\n",
    "    assert sample is None or 0 < sample <= 1\n",
    "\n",
    "    seed = seed if seed is not None else time.time()\n",
    "    rdd = rdd if sample is None else rdd.sample(False, sample, seed)\n",
    "\n",
    "    rddSortedWithIndex = (rdd.\n",
    "        sortBy(lambda x: x).\n",
    "        zipWithIndex().\n",
    "        map(lambda (x, i): (i, x)).\n",
    "        cache())\n",
    "\n",
    "    n = rddSortedWithIndex.count()\n",
    "    h = (n - 1) * p\n",
    "\n",
    "    rddX, rddXPlusOne = (\n",
    "        rddSortedWithIndex.lookup(x)[0]\n",
    "        for x in int(floor(h)) + np.array([0L, 1L]))\n",
    "\n",
    "    return rddX + (h - floor(h)) * (rddXPlusOne - rddX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.approxQuantile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
