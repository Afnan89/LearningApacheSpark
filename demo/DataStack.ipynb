{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Jointed Table\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.sparkContext.parallelize([\n",
    "    (\"assert\", Vectors.dense([1, 2, 3]))\n",
    "]).toDF([\"word\", \"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|  word|       vector|\n",
      "+------+-------------+\n",
      "|assert|[1.0,2.0,3.0]|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return (row.word, ) + tuple(float(x) for x in row.vector.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.rdd.map(extract).toDF([\"word\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+\n",
      "|  word| _2| _3| _4|\n",
      "+------+---+---+---+\n",
      "|assert|1.0|2.0|3.0|\n",
      "+------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
    "                     (4, 5, 6, 'd e f'),\n",
    "                     (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "new = df.withColumn('col4',explode(split('col4',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   2|   3|   a|\n",
      "|   1|   2|   3|   b|\n",
      "|   1|   2|   3|   c|\n",
      "|   4|   5|   6|   d|\n",
      "|   4|   5|   6|   e|\n",
      "|   4|   5|   6|   f|\n",
      "|   7|   8|   9|   g|\n",
      "|   7|   8|   9|   h|\n",
      "|   7|   8|   9|   i|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.sparkContext.parallelize([(1, 2, 3, 'a,b,c'),\n",
    "                     (4, 5, 6, 'd,e,f'),\n",
    "                     (7, 8, 9, 'g,h,i')]).toDF(['col1', 'col2', 'col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = df.withColumn('col4',explode(split('col4',',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   2|   3|   a|\n",
      "|   1|   2|   3|   b|\n",
      "|   1|   2|   3|   c|\n",
      "|   4|   5|   6|   d|\n",
      "|   4|   5|   6|   e|\n",
      "|   4|   5|   6|   f|\n",
      "|   7|   8|   9|   g|\n",
      "|   7|   8|   9|   h|\n",
      "|   7|   8|   9|   i|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = spark.sparkContext.\\\n",
    "    parallelize([['a', 'foo'], ['b', 'hem'], ['c', 'haw']]).toDF(['a_id', 'extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|a_id|extra|\n",
      "+----+-----+\n",
      "|   a|  foo|\n",
      "|   b|  hem|\n",
      "|   c|  haw|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = spark.sparkContext.parallelize([['p1', 'a'], ['p2', 'b'], ['p3', 'c']]).toDF([\"other\", \"b_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|other|b_id|\n",
      "+-----+----+\n",
      "|   p1|   a|\n",
      "|   p2|   b|\n",
      "|   p3|   c|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = a.join(b, a.a_id == b.b_id,'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----+\n",
      "|a_id|extra|other|b_id|\n",
      "+----+-----+-----+----+\n",
      "|   c|  haw|   p3|   c|\n",
      "|   b|  hem|   p2|   b|\n",
      "|   a|  foo|   p1|   a|\n",
      "+----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tqdm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-526593284c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named tqdm"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(int(1e4))):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# create an RDD of 100 random numbers\n",
    "x = [random.normalvariate(0,1) for i in range(100)]\n",
    "rdd = spark.sparkContext.parallelize(x)\n",
    "\n",
    "# plot data in RDD - use .collect() to bring data to local\n",
    "num_bins = 50\n",
    "n, bins, patches = plt.histogram(rdd.collect(), num_bins, normed=1, facecolor='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's use UCLA's college admission dataset\n",
    "file_name = \"http://www.ats.ucla.edu/stat/data/binary.csv\"\n",
    "\n",
    "# Creating a pandas dataframe from Sample Data\n",
    "pandas_df = pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "# Creating a Spark DataFrame from a pandas dataframe\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
